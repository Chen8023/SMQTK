{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to retrieve the relevant images from a set of ads with assigned clusters.\n",
    "\n",
    "\n",
    "### Input\n",
    "The input is specified by `DATA_FILE`, which is a JSON lines file containing CDR ad documents that each contain a `doc_id` and a `cluster_id`.\n",
    "\n",
    "### Output \n",
    "There are many resultant CSV files using for intermediary mapping, but the import end result is `CP1_clusters_ads_images.csv` with records following the format of `cluster_id,ad_id,sha1` along with the `CP1_imageset` directory which is SHA1 content addressed.\n",
    "\n",
    "#### Requirements\n",
    "##### pip\n",
    "- requests\n",
    "- certifi\n",
    "- elasticsearch\n",
    "- elasticsearch-dsl\n",
    "\n",
    "##### system\n",
    "- GNU Parallel\n",
    "- File utility\n",
    "\n",
    "##### Other\n",
    "- `../scripts/es_config.json` must be configured to point to the correct Elasticsearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_FILE = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!md5sum $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract CDR IDs from DATA_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdr_ids = set()\n",
    "\n",
    "with open(DATA_FILE, 'r') as infile:\n",
    "    for line in infile:\n",
    "        ad = json.loads(line.strip())\n",
    "        cdr_ids.add(ad['doc_id'])\n",
    "        \n",
    "with open('../data/CP1_cdr_ids.txt', 'w') as outfile:\n",
    "    for cdr_id in cdr_ids:\n",
    "        outfile.write('%s\\n' % cdr_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is responsible for retrieving the documents from Elasticsearch whose parents are in `cdr_ids` (i.e. the images which belong to these ads).\n",
    "\n",
    "A few notes about `parallel`:\n",
    "- The commands run and their exit status are stored in the file specified as --joblog\n",
    "   - This means the failing jobs could be retrieved using something like    \n",
    "      `awk '$7 != 0' ../data/misc/get_es_child_documents.log`\n",
    "- Depending on the system, -j and --max-args may need to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!time parallel -j6 --joblog ../data/misc/get_es_child_documents.log \\\n",
    "                   --arg-file ../data/CP1_cdr_ids.txt \\\n",
    "                   --retries 3 \\\n",
    "                   --timeout 20 \\\n",
    "                   --max-args 150 \\\n",
    "                   python ../scripts/get_es_child_documents.py | sort --unique > ../data/CP1_image_documents.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of particular interest from the documents are the values in the `obj_stored_url` field, write these to a separate file for downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_urls = set()\n",
    "\n",
    "with open('../data/CP1_image_documents.json', 'r') as infile:\n",
    "    for line in infile:\n",
    "        image_doc = json.loads(line)\n",
    "        \n",
    "        if image_doc['obj_stored_url']:\n",
    "            image_urls.add(image_doc['obj_stored_url'])\n",
    "        \n",
    "# Construct file for parallel downloading\n",
    "with open('../data/CP1_image_urls.txt', 'w') as outfile:\n",
    "    for url in image_urls:\n",
    "        outfile.write('%s\\n' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same notes for parallel above follow here.\n",
    "\n",
    "This will retrieve the URLs and store them as SHA1 addressed filenames and print the mapping between URLs and SHA1 hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time parallel -j50 \\\n",
    "               --joblog ../data/misc/dl_images.log \\\n",
    "               --arg-file ../data/CP1_image_urls.txt \\\n",
    "               --retries 3 \\\n",
    "               --timeout 20 \\\n",
    "               python ../scripts/download_url_as_sha.py | sort --unique > ../data/CP1_url_sha.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is responsible for deleting invalid files which are defined as files with a size of 0, or files which don't contain `\"image data\"` as per the `file` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "for (dirpath, _, filenames) in os.walk('../data/CP1_imageset'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirpath, filename)\n",
    "        exists = os.path.exists(path)\n",
    "        if exists and os.stat(path).st_size == 0:\n",
    "            os.unlink(path)\n",
    "        elif exists:\n",
    "            if 'image data' not in subprocess.check_output(['file', path]):\n",
    "                os.unlink(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the rows that are still valid, so `CP1_url_sha.txt` can be adjusted to reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_rows = []\n",
    "\n",
    "with open('../data/CP1_url_sha.txt', 'r') as infile:\n",
    "    reader = csv.reader(infile, delimiter=' ')\n",
    "\n",
    "    for row in reader:\n",
    "        url, sha = row\n",
    "        \n",
    "        if os.path.isfile(os.path.join('../data/CP1_imageset', sha[:3], sha)):\n",
    "            valid_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/CP1_url_sha.txt', 'w') as outfile:\n",
    "    writer = csv.writer(outfile, lineterminator='\\n', delimiter=' ')\n",
    "    \n",
    "    for row in valid_rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n",
    "\n",
    "The rest of the notebook is about writing `../data/CP1_clusters_ads_images.csv` given the data we have already obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CP1_image_documents.json gives ad_id and image doc_id which can be mapped to sha via image_doc_id_sha\n",
    "clusters_ads = set()\n",
    "\n",
    "# cluster_id,ad_id are given from the official DATA_FILE\n",
    "with open(DATA_FILE, 'r') as infile:\n",
    "    for line in infile:\n",
    "        ad = json.loads(line.strip())\n",
    "        clusters_ads.add((ad['cluster_id'], ad['doc_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d ads across %d clusters.' % (len(set([x[1] for x in clusters_ads])),\n",
    "                                      len(set([x[0] for x in clusters_ads]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the sha1 through the CP1_url_shas.txt through the obj_stored_url in CP1_image_documents.json\n",
    "# Going from image to ad will always yield an ad\n",
    "with open('../data/CP1_image_documents.json', 'r') as infile:\n",
    "    ad_id_image_urls = defaultdict(set)\n",
    "    \n",
    "    for line in infile:\n",
    "        image_doc = json.loads(line)\n",
    "        \n",
    "        if not isinstance(image_doc['obj_parent'], list):\n",
    "            image_doc['obj_parent'] = [image_doc['obj_parent']]\n",
    "                \n",
    "        for ad_id in image_doc['obj_parent']:\n",
    "            ad_id_image_urls[ad_id].add(image_doc['obj_stored_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d image URLs exist across %d ads.' % (sum([len(x) for x in ad_id_image_urls.values()]), len(ad_id_image_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go from obj_stored_url to sha1\n",
    "with open('../data/CP1_url_sha.txt', 'r') as infile:\n",
    "    url_sha = {}\n",
    "    \n",
    "    for line in infile:\n",
    "        url, sha = line.strip().split()\n",
    "        url_sha[url] = sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%d unique URLs/images.' % len(url_sha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('../data/CP1_clusters_ads_images.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    # Headers\n",
    "    writer.writerow(['cluster_id', 'ad_id', 'sha1'])\n",
    "    \n",
    "    for (cluster_id, ad_id) in clusters_ads:\n",
    "        # Finding a url should always work\n",
    "        # Finding the sha from a url may not, if the URL failed to be retrieved (404, whatever)\n",
    "        image_urls_from_ad = ad_id_image_urls[ad_id] \n",
    "        image_shas_from_ad = set([url_sha[x] for x in image_urls_from_ad if x in url_sha])\n",
    "        \n",
    "        for sha1 in image_shas_from_ad:\n",
    "            writer.writerow((cluster_id, ad_id, sha1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
